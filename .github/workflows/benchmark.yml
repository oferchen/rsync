name: Benchmark

on:
  push:
    tags:
      - "v[0-9]*.[0-9]*.[0-9]*"
  workflow_dispatch:
    inputs:
      baseline_tag:
        description: 'Baseline tag/branch to compare against'
        required: false
        default: ''

permissions:
  contents: write
  pull-requests: write

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-D warnings"

jobs:
  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo builds
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: benchmark

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential autoconf automake libtool \
            libacl1-dev libattr1-dev zlib1g-dev libxxhash-dev libzstd-dev liblz4-dev

      - name: Build oc-rsync (release)
        run: cargo build --release --workspace

      - name: Cache upstream rsync
        id: cache-rsync
        uses: actions/cache@v4
        with:
          path: target/interop/upstream-src/rsync-3.4.1
          key: upstream-rsync-3.4.1-${{ runner.os }}-built

      - name: Build upstream rsync 3.4.1
        if: steps.cache-rsync.outputs.cache-hit != 'true'
        run: |
          set -euo pipefail
          mkdir -p target/interop/upstream-src
          cd target/interop/upstream-src

          if [ ! -d rsync-3.4.1 ]; then
            curl -L https://download.samba.org/pub/rsync/src/rsync-3.4.1.tar.gz | tar xz
          fi

          cd rsync-3.4.1
          if [ ! -f rsync ]; then
            ./configure --disable-xxhash --disable-zstd --disable-lz4
            make -j$(nproc)
          fi

      - name: Run criterion benchmarks
        id: criterion
        run: |
          # Run internal optimization benchmarks
          cargo bench -p checksums --bench checksums_benchmark -- --noplot > /dev/null 2>&1 || true
          cargo bench -p engine --features optimized-buffers --bench optimizations_benchmark -- --noplot 2>&1 | tee criterion_output.txt || true

          # Extract key metrics for buffer pool
          echo "## Internal Optimization Benchmarks" > criterion_report.md
          echo "" >> criterion_report.md
          echo "### Buffer Pool Performance" >> criterion_report.md
          echo "" >> criterion_report.md
          echo '```' >> criterion_report.md
          grep -A1 "buffer_allocation\|sequential_buffer_ops\|concurrent_buffer" criterion_output.txt | head -50 >> criterion_report.md || echo "No buffer pool benchmarks found" >> criterion_report.md
          echo '```' >> criterion_report.md

      - name: Run benchmark
        id: benchmark
        run: |
          cat << 'PYEOF' > /tmp/benchmark.py
          #!/usr/bin/env python3
          import subprocess
          import tempfile
          import shutil
          import time
          import json
          import os
          import sys

          UPSTREAM = "target/interop/upstream-src/rsync-3.4.1/rsync"
          OC_RSYNC = "target/release/oc-rsync"

          def benchmark(cmd, runs=5):
              times = []
              for _ in range(runs):
                  start = time.perf_counter()
                  subprocess.run(cmd, shell=True, capture_output=True)
                  elapsed = time.perf_counter() - start
                  times.append(elapsed)
              return {
                  "mean": sum(times) / len(times),
                  "min": min(times),
                  "max": max(times),
              }

          def main():
              tmpdir = tempfile.mkdtemp(prefix="rsync_bench_")
              results = {"tests": [], "summary": {}}

              try:
                  src = f"{tmpdir}/src"
                  dst_up = f"{tmpdir}/dst_upstream"
                  dst_oc = f"{tmpdir}/dst_oc"

                  os.makedirs(f"{src}/small", exist_ok=True)
                  os.makedirs(f"{src}/medium", exist_ok=True)
                  os.makedirs(f"{src}/large", exist_ok=True)

                  # Create test data
                  print("Creating test data...", file=sys.stderr)

                  # Small files (1000 Ã— 1KB)
                  for i in range(1000):
                      with open(f"{src}/small/file_{i}.txt", 'wb') as f:
                          f.write(os.urandom(1024))

                  # Medium files (100 Ã— 100KB)
                  for i in range(100):
                      with open(f"{src}/medium/file_{i}.bin", 'wb') as f:
                          f.write(os.urandom(100 * 1024))

                  # Large files (10 Ã— 10MB)
                  for i in range(10):
                      with open(f"{src}/large/file_{i}.dat", 'wb') as f:
                          f.write(os.urandom(10 * 1024 * 1024))

                  total_size = sum(os.path.getsize(os.path.join(dp, f))
                                  for dp, dn, fn in os.walk(src) for f in fn)
                  total_files = sum(len(fn) for _, _, fn in os.walk(src))

                  results["test_data"] = {
                      "size_mb": round(total_size / 1024 / 1024, 1),
                      "files": total_files
                  }

                  def reset_dst():
                      shutil.rmtree(dst_up, ignore_errors=True)
                      shutil.rmtree(dst_oc, ignore_errors=True)
                      os.makedirs(dst_up, exist_ok=True)
                      os.makedirs(dst_oc, exist_ok=True)

                  tests = [
                      ("initial_sync", "Initial sync (-av)", f"-av {src}/ {{dst}}/", True),
                      ("nochange_sync", "No-change sync (-av)", f"-av {src}/ {{dst}}/", False),
                      ("checksum_sync", "Checksum sync (-avc)", f"-avc {src}/ {{dst}}/", False),
                      ("dryrun", "Dry-run (-avn)", f"-avn {src}/ {{dst}}/", False),
                      ("delete_sync", "Delete sync (--delete)", f"-av --delete {src}/ {{dst}}/", False),
                      ("large_files", "Large files (100MB)", f"-av {src}/large/ {{dst}}/", True),
                      ("small_files", "Small files (1000Ã—1KB)", f"-av {src}/small/ {{dst}}/", True),
                  ]

                  for test_id, name, args, do_reset in tests:
                      print(f"Running: {name}...", file=sys.stderr)

                      if do_reset:
                          reset_dst()

                      up_args = args.format(dst=dst_up)
                      oc_args = args.format(dst=dst_oc)

                      up_result = benchmark(f"{UPSTREAM} {up_args}")
                      oc_result = benchmark(f"{OC_RSYNC} {oc_args}")

                      ratio = oc_result["mean"] / up_result["mean"] if up_result["mean"] > 0 else 0

                      results["tests"].append({
                          "id": test_id,
                          "name": name,
                          "upstream": up_result,
                          "oc_rsync": oc_result,
                          "ratio": round(ratio, 2)
                      })

                  # Calculate summary
                  ratios = [t["ratio"] for t in results["tests"]]
                  results["summary"] = {
                      "avg_ratio": round(sum(ratios) / len(ratios), 2),
                      "best_ratio": round(min(ratios), 2),
                      "worst_ratio": round(max(ratios), 2),
                  }

                  print(json.dumps(results, indent=2))

              finally:
                  shutil.rmtree(tmpdir, ignore_errors=True)

          if __name__ == "__main__":
              main()
          PYEOF

          python3 /tmp/benchmark.py > benchmark_results.json

          # Generate markdown report
          python3 << 'MDEOF'
          import json

          with open("benchmark_results.json") as f:
              data = json.load(f)

          print("## Benchmark Results\n")
          print(f"Test data: {data['test_data']['size_mb']}MB ({data['test_data']['files']} files)\n")
          print("| Test | Upstream | oc-rsync | Ratio |")
          print("|------|----------|----------|-------|")

          for t in data["tests"]:
              up = t["upstream"]["mean"]
              oc = t["oc_rsync"]["mean"]
              ratio = t["ratio"]
              emoji = "ðŸŸ¢" if ratio < 1.0 else "ðŸŸ¡" if ratio < 2.0 else "ðŸ”´"
              print(f"| {t['name']} | {up:.3f}s | {oc:.3f}s | {emoji} {ratio:.2f}x |")

          print(f"\n**Summary:** Average ratio: {data['summary']['avg_ratio']}x")
          print(f"- Best: {data['summary']['best_ratio']}x")
          print(f"- Worst: {data['summary']['worst_ratio']}x")
          print(f"\n_Ratio < 1.0 = oc-rsync faster, > 1.0 = upstream faster_")
          MDEOF > benchmark_report.md

          cat benchmark_report.md

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          retention-days: 90
          path: |
            benchmark_results.json
            benchmark_report.md
            criterion_report.md
            criterion_output.txt

      - name: Add benchmark to release notes
        if: startsWith(github.ref, 'refs/tags/')
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TAG="${GITHUB_REF##*/}"

          # Get current release body
          CURRENT_BODY=$(gh release view "$TAG" --json body -q .body 2>/dev/null || echo "")

          # Append benchmark reports
          CRITERION_REPORT=""
          if [ -f criterion_report.md ]; then
            CRITERION_REPORT=$(cat criterion_report.md)
          fi

          NEW_BODY="${CURRENT_BODY}

          ---

          $(cat benchmark_report.md)

          ---

          ${CRITERION_REPORT}"

          # Update release
          gh release edit "$TAG" --notes "$NEW_BODY" || echo "Could not update release notes"

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('benchmark_report.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
